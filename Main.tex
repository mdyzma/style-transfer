\documentclass[a4paper,conference]{IEEEtran}
%
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{array}
\usepackage{float}
\usepackage{amsmath,bm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{slashbox}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{arydshln}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{url}
\usepackage{stackengine}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\newcommand\IncG[2][]{\addstackgap{%
  \raisebox{-.5\height}{\includegraphics[#1]{#2}}}}



% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\newcommand{\ResidualLayerNumber}{\beta}


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Heritage Portal - Real time video neural style transfer on mobile devices}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{
\IEEEauthorblockN{Wojciech Dudzik}
\IEEEauthorblockA{Netguru S.A., \\Poznań, Poland \\
Email: wojciech.dudzik@netguru.com}
\and
\IEEEauthorblockN{Damian Kosowski}
\IEEEauthorblockA{Netguru S.A., \\Poznań, Poland \\
Email: damian.kosowski@netguru.com}
}



% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Neural style transfer is a well known branch of deep learning research with many interesting works and two major drawbacks. Most of the works in the field are hard to use by non expert user and substantial hardware resources are required. In this work we present a solution to both of these problems. We have applied neural style transfer for real time video (over 25 frames per second), which is capable of running on mobile devices. We also investigate the works on achieving temporal coherence and present the idea of fine-tuning, already trained models, to achieve stable video. We also analyze the impact of the common deep neural network  architecture on the performance on mobile devices with regard to number of layers and filters present. In the experiment section we present the results of our work with respect to the iOS devices and discuss the problems present in current Android devices as well as future possibilities. At the end we present the qualitative results of stylization and quantitative results of performance tested on iPhone 11 Pro. The presented work is incorporated in Heritage Portal application available at Apple's App Store.
\end{abstract}


%https://github.com/huihuangzhao/Neural-Style-Transfer-Papers-Code

\section{Introduction}

Paintings as a form of art was  accompanying us through the history presenting all kind of things from the might kings portraits through, historic battles to ordinary daily activities. It all changed with the invention of photography and later the digital photography. Nowadays most of us carry a smart phone equipped with HD camera. In the past, re-drawing an image in a particular style required a well-trained artist and lots of time (and money).  

This problem has been studied by both artist and computer science researchers for over two decades within the field of non-photorealistic rendering (NPR). However, most of these NPR stylisation algorithms were designed for particular artistic styles and could not be easily extended to other styles. Common limitation of these methods is that they only use low-level image features and often fail to capture image structures effectively. The first to use convolution neural networks (CNN) for that task was Gatys et. al. \cite{Gatys2016ImageST,GatysEB15archivix}. They proposed a neural algorithm for automatic image style transfer, which refines a random noise to a stylized image iteratively constrained by a content loss and a style loss. 
%% To zdanie jest niejasne chodzi o to, ze ktos rozwinal ich pomysl uzycia CNN? Moze cos takiego:
% Tis approach resulted in multiple works, which attempted to improve original and addressed it's major drawbacks such as: i tu wymienic.

This led to multiple works that followed the original and improved its drawbacks. 
%% Ja bym rozwinal skrot i wyjasnil go tutaj, czyli tam gdzie zostal uzyty po raz pierwszy (zelazna regula pisania pubklikacji). "We made it possible" brzmi dziwnie. We focused on... We studied possibility...
In our work we made it possible to use these NST method for videos on mobile phone, especially on iphone's. 
% Rozbilbym to na dwa zdania i dodal opis do czego uzyliscie wiedzy z badania czasowej zbieznosci(?) istniejacych metod.
In order to do that we investigate the problem of temporal coherence among existing methods.  and refined the neural network architecture, therefore  our main contribution included:

\begin{itemize}
\item Real time application of neural style transfer to videos on mobile devices (iOS),
\item Investigation on achieving temporal coherence with existing methods,
\item Analyzing the size of the models present in literature, and proposing new smaller architecture for the task.
\end{itemize}

First we shall review current status of NST field related to image and video style transfer (see \ref{sec:related-work}). Further in (section \ref{sec:methods}) we describe training regime and proposed neural network architecture.  And finally, achieving temporal coherence will be presented in Section \ref{sec:}. In Section \ref{sec:experiments}, we will discuss results obtained during out experiment and show performance on mobile devices.

%Image style transfer is a task that aims to render the content of one
%image with the style of another,

  
\section{Related work}
\label{related-work}

In this section we will briefly review the selected methods for NST related to our work. For more comprehensive review we recommend \cite{NeuralStyleTransferReview}.  
% Pomieszanie czasow. Zdecydij sie na past simple. Present nie brzmi dobrze w publikacji, tym bardziej, ze opisujesz cos co juz bylo.
The first method for NST was proposed by Gatys et. al. \cite{Gatys2016ImageST}. He demonstrated very exciting results which catches eyes in both academia and industry. That method opened many new possibilities and attracted attention of other researchers i.e.: \cite{JohnsonAL16,DumoulinSK16,Champandard16,LuanPSB17,GatysEBHS16}, who's work based on Gatys original idea. One of the best successory projects was proposed by Johnson et al. \cite{JohnsonAL16} with the feed-forward perceptual losses model. In his work, Johnson  used a pre-trained VGG \cite{Simonyan15} to compute 
% NIE THEM, WYJASNIJ CO OBLICZYL
them. This allowed real-time inference speed while maintaining good style quality.  A natural way to extend this technique to videos is to perform a similar image transformation frame by frame. Unfortunately, this scheme, inevitably brings temporal inconsistencies and thus causes severe flicker artifacts if applied to single image transformation. One of techniques useful in solving this issue is approach proposed by of Ruder et al. \cite{RuderDB16} which is specifically designed for video style transfer. Despite its usability for video, it requires a time consuming computations (dense optical flow calculation), and may take several minutes to process a single frame. Due to this fact it makes it not applicable for real-time usage. To obtain a consistent and fast video style transfer method, some real-time or near real-time models have recently been developed. 

Using a feed-forward network design, Huang et. al. \cite{Huang_2017_CVPR} proposed a model similar to the Johnson's approach \cite{JohnsonAL16} with an additional temporal loss. This model provides faster inference times, since it neither estimates optical flows nor uses information about the previous frame in the inference stage.  Another, more recent, development published by  Gao et. al. \cite{Reconet} describes model, which does not estimate optical flows but involves ground-truth optical flows only in loss calculation in the training stage. The use of the ground-truth optical flow allows to obtain occlusion mask. Masks are further used to mark regions that are unchanged between frames. Additionally, temporal loss is considered not only on output image but also at the feature level of DNN. Gao's lightweight and feed-forward network can be considered to be the one of the fastest approach for video NST.

% Strasznie zamotane. Trzeba to rozbic na kilka zdan i zastosowac mniej kwiecisty styl. Np. Still applying methods mentioned above may be troublesome due to limited capabilities of mobile devices. Even though modern smartphones are able to .... bla bla bla ...
On the other hand applying these methods for achieving real-time neural style transfer on mobile devices (i.e. smartphones) introduce more strict requirements about computation capabilities. There was several noticeable reports dealing with this issue, i.e. \cite{TinyTransformer,iosApp,Pictory}. In this paper authors are focusing on running and optimizing 
% Jakie metody? Nic nie wiadomo. Jedno zdanie wiecej by sie przydalo
current methods on mobile platforms using specialized ARM instructions - NEON or 
% Rozbic na dwa zdania
 GPU while all of them perform only image to image style transfer.
Other implementations include \cite{Novecento} and very popular Prisma application. Both of them rely on processing images on server side (although Prisma added later option for offline processing). As a consequence both of them heavily depend on internet connection and enable processing of single images at the same time. 

Since pace of progression
% Progression in what? Dodalbym, ze chodzi o mozliwosci hardweru i nowsze procki
 in mobile devices is very fast, the computation capabilities grows each year. This 
 
% trend was shown clearly in ...
can be clearly seen in Ignatov et. al\cite{aiMobileBenchmark} where authors present a comprehensive review on smartphones performance against popular machine learning and deep learning technologies used nowadays. In particular they analyzed devices with Android system. The introduction of 4-th generation (according to 
% Whose? No i trzeba to rozbic na krotsze zdania...
their designations) neural processing units (NPU) for Android phones as well as unifying programming interface with frameworks like Tensorflow Lite, Android NNAPI or CoreML (for iPhones) makes it easier for any deep neural network application. Before that deploying a single model to multiple devices was difficult since one would need to integrate it with multiple different SDKs prepared by each chip manufacturer e.g. MNN, Mace. Each of them provided also certain limitations in term of supported operations, model quantization options and hardware usage.

This motivated us to pursue possibilities for work involved in both areas of interest: NST and mobile applications of DNN. As a result we propose 

% Nieco inaczej. TUTAJ JEST ZBYT OGOLNIKOWO.
improvements by combining the methods from those fields and propose a real time video neural style transfer on mobile devices.


 
\section{Proposed Method}
\label{methods}

We propose a reliable method for achieving real-time neural style transfer on mobile devices. In our approach we are primarily focused on iPhones. There are still some differences between them and Android based phones which we will address in Section \ref{Andorid_and_ios}. In this section we will present our network architecture and training procedure. 

\subsection{Network architecture}

Network architecture can be seen on Fig. \ref{networkArchitecture}. It is based on the architecture presented in \cite{Reconet}. 
% NIe wiem o co chodzi. Trzeba to napisac inaczej, np.: In this paper, Reconet proposed sveral layers of ... bla, bla, bla.. Detailed representation/description of eacjh layer (most important layers) of the network was presented in table...
In the \cite{Reconet} paper the architecture of the network with regard to layers sizes was as presented in Tab. \ref{tableWithArchitecture}.

We provide modifications including changes in number of filters for each layer (showed in last column of Tab. \ref{tableWithArchitecture}), removing the TanH operation at the last layer and ReLU operations in the up-sampling part of the network. Moreover, all of the kernels size was equal $3 \times 3$. We used reflection padding for each of the convolutions. For up-sample layer we used nearest neighbors method. Visualization of residual layer architecture is presented in Fig. \ref{residualBlockArchitecture}. The $\ResidualLayerNumber$ is introduced in order to investigate the influence of the number of residual layers on the final result of stylization. 


%moze dodac cytowanie do residual layer
\begin{figure}[ht]
\caption{Architecture of residual block}
\label{residualBlockArchitecture}
\centering
\includegraphics[scale=0.5]{figures/ResidualBlock.png}
\end{figure}



%Tabela z architekturą tutaj oryginał versus ta nasza
\begin{table}[ht]
\scriptsize
\caption{Detailed layer by layer architecture}
\label{tableWithArchitecture}
\centering
\begin{tabular}{cccc}
\hline 
Layer & Type & Filters in \cite{Reconet} & Our filters \\ 
\hline 
1  & Input &  &  \\ 
\hline 
2 & Conv + instnorm + Relu & $ 48$  & $\alpha \cdot 32$ \\ 
\hline 
3 & Conv + instnorm + Relu (stride 2) & $96$ & $\alpha \cdot 48$ \\ 
\hline 
4& Conv + instnorm + Relu (stride 2) & $192$ & $\alpha \cdot 64$ \\ 
\hline 
5 & Residual  $\times  4 \cdot \ResidualLayerNumber$   & $192$ &  $\alpha \cdot 64$ \\ 
\hline
6 & Upsample & &\\
\hline 
7 & Conv + instnorm & $96$ & $\alpha \cdot 48$ \\ 
\hline 
8 & Upsample & &\\
\hline 
9 & Conv + instnorm & $48$ & $\alpha \cdot 32$ \\ 
\hline 
10 & Conv  & $3$ & $3$ \\ 
\hline 
\end{tabular} 
\end{table}


\begin{figure*}[ht]
\caption{Reconet architecture \cite{Reconet}.}
\label{networkArchitecture}
\centering
\includegraphics[width=0.95\textwidth]{figures/reconet_architecture.png}
\end{figure*}

%Convolutional layers use filter sizes of 3 × 3, padding of 1, and stride of 1.
%– The rectified linear unit (ReLU) layer is an elementwise function ReLU(x) = max{x, 0}.
%– The instance norm (InstanceNorm) layer standardizes each feature channel independently to have 0 mean and a standard
%deviation of 1. This layer has shown impressive performance in image generation networks [34].
%– Maxpooling layers downsample by a factor of 2 by using filter sizes of 2 × 2 and stride of 2.
%– Nearest neighbor (NN) upsampling layers upsample by a factor of 2 by using filter sizes of 2 × 2 and stride of 2.


\subsection{Training procedure}

As there is no code published by authors of \cite{Reconet}, we tracked an open source implementation at GitHub webpage\footnote{\url{https://github.com/EmptySamurai/pytorch-reconet}}. As far as we know this implementation closely reassembles the work presented in the paper, but shows additional artifacts (in forms of white bubbles) that were not mentioned in original work. This phenomena are presented in Fig. \ref{fig:problems}, where artifacts can be easily spotted. An interesting and detailed explanation to this problem was presented in \cite{karras2019analyzing}. Authors discovered that artifacts appear because of instance normalization employed to increse training speed. It is known that the latter is considered a better altrnative than batch normalization \cite{instanceNormalization} for style transfer tasks. Solution proposed by the author of this repository is to incorporate filter response normalization \cite{FRNlayer}. All instance normalization are replaced by filter response normalization (FRN) and all ReLU operations are replaced by threshloded linear unit (TLU). Despite the fact that this solution removes problematic artifacts, still it can not be deployed on mobile devices, because FRN is not yet supported by any major framework at the time of writing. Of course it can be computed on the general CPU but this results in major performance drop.

On the other hand our implementation of ReCoNet in particular cases may present faded colors, as seen in Fig. \ref{fig:problems}b and final effect may not as good as expected (\ref{fig:problems}a). However... 

% Do przepisania. Nie wiadomo o co chodzi
This expected result is achieved by the same network as in \ref{fig:problems}b-c but trained without temporal loss element. In our opinion there might be some implementation details that we missing which are crucial for reaching the desired result. Thus we came with an idea of two stage training as follow:


%Although style present in Fig. \ref{fig:problems}a is different than Fig. \ref{fig:problems}c we are not discussing 

\begin{figure*}[ht]
%\begin{tabular}{p{.3\textwidth}p{.3\textwidth}p{.3\textwidth}}
\begin{tabular}{ccc}


\IncG[width=.3\textwidth]{expected.png}  &
\IncG[width=.3\textwidth]{badcolors.png} &
\IncG[width=.3\textwidth]{artifacts.png}   
\\
a) Expected & b) Bad colors & c) Artifacts 

\end{tabular}
\caption{Problems that turned out on achieving temporal coherence}
\label{fig:problems}
\end{figure*}


\begin{enumerate}
\item Training only with content and style loss as in \cite{JohnsonAL16},
\item Fine-tuning network with feature and output temporal losses added (losses came from  \cite{Reconet}).
\end{enumerate}

During the first stage of the training we adopt the content loss $\mathcal{L}_{content}$, the style loss  $\mathcal{L}_{style}$ and the total variation regularizer  $\mathcal{L}_{tv}$ in perceptual losses model based on \cite{JohnsonAL16}. The final form of the loss function for the first stage is:

\begin{equation}
 \mathcal{L} (t) = \alpha \mathcal{L}_{content} + \beta \mathcal{L}_{style}  + \gamma \mathcal{L}_{tv}
\end{equation}


During the second stage of fine-tuning and achieving temporal consistency we adopt the solution presented in \cite{Reconet}, where  

\begin{equation}
\begin{aligned}
 \mathcal{L} (t-1,t) =  \sum_{i \in \{t-1, t\}} \left( \alpha \mathcal{L}_{content}(i) + \beta \mathcal{L}_{style}(i)  + \gamma \mathcal{L}_{tv}(i) \right) \\ + \lambda_{f} \mathcal{L}_{temp,f}(t-1,t) + \lambda_{o} \mathcal{L}_{temp,o}(t-1,t)
 \end{aligned}
\end{equation}

where $\lambda_{f} \mathcal{L}_{temp,f}(t-1,t) + \lambda_{o} \mathcal{L}_{temp,o}(t-1,t)$ are feature temporal and output temporal loss components respectively, as presented in \cite{Reconet}. The key idea in achieving temporal consistency is the use of optical flow information between the consecutive frames and occlusion masks, for marking the areas which should stay the same in between frames. By doing this during the training we are able to provide model that do not need these information during inference stage, thus making it a lot faster, as estimating dense optical flow is operationaly expensive (time and computational wise). The content loss and the style loss utilize feature maps at relu2\_2 layer and [relu1\_2, relu2\_2, relu3\_3, relu4\_3] layers respectively. We used VGG16 pretrained in ImageNet for calculating all of the losses.





%\textcolor{red}{Tutaj można dodać wyjaśnienie poszczególnych lossów jako streszczenie ReCoNet - głowne idee i równania bo w tej chwili trochę mało to mówi jak nie przeczyta się innych papierów}

%\subsection{CoreML and AR Core}
%\item CoreML support with the iPhone dedicated NPU - ease of use 
%\item Talk about AR - section that can be written by Milan Sawicki

%\textcolor{red}{Will be prepared by Milan Sawicki after application release}



%\begin{itemize}
%\item Discuss the problems on Android platform which stopped us from deploying the application (yet) implementation in Pytorch instead of tensorflow
%\item problems with model conversion between formats (with regard to the performance side - added transpositions in the process)
%\item Pytorch lack of GPU/NNAPI support (for now)
%\item Tensorflow Lite limitations with GPU (limited set of operations)
%\item Tensorflow Lite policy about switching to CPU in case of unsupported  operation with GPU/NNAPI
%\item Lack of drivers and documentation of NNAPI as it heavily depended on the hardware (not all operations are supported, some chips needs DNN to be quantized and so on, reference the AI benchmark with 1-4th  NPU generations)
%\item
%\end{itemize}



\section{Experiments and Discussion}
\label{experiments}

%\item Describe the solution for style transfer on iOS how we did that, measure the performance, show some examples (links to some videos would be grate, both in term of iOS application and style transfer model)
%\item The measurement of performance of DNN vs. the rest of application (maybe battery drain)

For the training of network we used Coco \cite{cocoDataset} and MPI Sintel \cite{Mpi_sintel}  datasets. All frames were resized to 256 × 256 px for first stage of the training. In second phase we used MPI Sintel, which required 640x360 resolution. We implement our style transfer pipeline on PyTorch 1.3  with Cuda 10.1 and cuDNN 7.1.4. All of the trainings were performed on a single GTX 1080 GPU. In next subsections we will discuss the differences between deploying DNN on Android and iPhone system.




Results of two stage training procedure are depicted on Fig. \ref{fig:differentStyle}. First row presents style images, while the second row presents the result of the model after the first phase of training. In the third row we present the same images after incorporating fine-tuning for stabilization of the video. As we can see, especially on example of Weeping Woman and Scream introducing temporal coherence into the model weakened the style that was learned by the neural network. 

% Nie rozumiem
This encompasses mainly of smoothing out the resulting image so the colors are more unified across wide area of image. As presented achieving the desired stabilization introduce some trade-off in case of some particular styles. As an advantage over other methods our two stage training approach can provide both models (with and without stabilization) thus letting the author decide what effect is desired.

\begin{figure*}[ht]
%\begin{tabular}{p{.23\textwidth}p{.23\textwidth}p{.23\textwidth}p{.23\textwidth}}
\begin{tabular}{cccc}

\hline
a) Scream  & b) Mona Lisa & c) Weeping Woman & d) Starry Night \\
\hline
\IncG[width=.21\textwidth,height=.23\textwidth]{figures/style/scream.jpg} &
\IncG[width=.18\textwidth,height=.23\textwidth]{figures/style/MonaLisa.jpg} &
\IncG[width=.21\textwidth,height=.23\textwidth]{figures/style/picasso.jpg} &
\IncG[width=.23\textwidth,height=.21\textwidth]{figures/style/starry.jpg}
\\

\IncG[width=.23\textwidth,height=.18\textwidth]{figures/differentStyle/img_scream_small.jpg} &
\IncG[width=.23\textwidth,height=.18\textwidth]{figures/differentStyle/img_monalisa_small.jpg} &
\IncG[width=.23\textwidth,height=.18\textwidth]{figures/differentStyle/img_picasso_small.jpg} &
\IncG[width=.23\textwidth,height=.18\textwidth]{figures/differentStyle/img_starry_night_small.jpg}
\\

\IncG[width=.23\textwidth,height=.18\textwidth]{figures/differentStyle/img_after_scream_small.jpg} &
\IncG[width=.23\textwidth,height=.18\textwidth]{figures/differentStyle/img_after_monalisa_small.jpg} &
\IncG[width=.23\textwidth,height=.18\textwidth]{figures/differentStyle/img_after_picasso_small.jpg} &
\IncG[width=.23\textwidth,height=.18\textwidth]{figures/differentStyle/img_after_starry_night_small.jpg}
\\
\hline

\end{tabular}
\caption{Examples of different stylisation achieved with presented training technique. First row is style image, second is result after first phase of training, the last row presents final model results. }
\label{fig:differentStyle}
\end{figure*}



\subsection{iOS vs Android implementation}
\label{Andorid_and_ios}
% Differences, ale w czym...  trzeba to napisac w 2-3 slowach, nazwac problem. np.: ...in machine, deep learning frameworks implementation. czy cos w tym stylu.
There are some noticeable differences between Android and iOS devices. First of all Apple introduced dedicated NPU with the 
% A11 co?
A11 Bionic back in 2017. Although not available to 3rd party applications, the next generation 
% A12 co? trzeba nazwac rzeczy, nie kazdy wie, ze to chip
A12 changed that in 2018. This allowed for very fast and power efficient calculations with the dedicated NPU. 
% Co miales na mysli? Nie wiem co mialo wyrazic to zdanie.
What is more thanks to Apple closed ecosystem, they were able to provide the CoreML library that hides the technical details from developer. 

During multiple numericql experiments we have noticed that our neural network implementation worked fast also with Apple devices without dedicated NPU. This wa spossible due to the fatc that CoreML librar can also exploit the GPU. Switch between NPU and GPU on different devices is done automatically, depending on their hardware. Another advantage of the CoreML is that it supports a wide variety of formats as well as provides proper conversion mechanism for different models e.g ONNX $\rightarrow$ CoreML, Keras $\rightarrow$ CoreML. 


The Android devices are not so unified which brings a number of problems during development. The first to follow, there is no single library to cover multiple use-cases. In our opinion, most promising solution for DNN porjects is Tensorflow Lite, however this may change in the future.  Being relatively  mature library, TF Lite,  it is not a silver bullet. 

%  Troche pozmienialem, ale czytajac dalej prsestalem rozummiec o co w tym zdaniu chodzi.
It  depends on the backend used (CPU, GPU or dedicated hardware) different operations (layers) might be supported even to the point where the kernel of bigger size is not supported.

This involves an automatic mechanism of switching computation from the desired hardware to the CPU which often is the slowest possible option. These situations must be carefully examined and addressed. Another problem when using Tensorflow Lite is the conversions of models trained using different frameworks. Some of the conversion tools might introduce additional layers, which are incompatible with TF Lite standards (API??). For example, when converting Pytorch to Tensorflow through common format of ONNX. 

% Which ones to what input standard? Trzeba to nazwac.
As these frameworks depends on the different layout of data (NHWC vs NCHW) any kind of conversion adds transform layers which significantly impact networks performance. Numerical experimets with conversion (not published) showed, that our application noted 30-40\% drop in the frames per second.
Another, worth mentioning, library is Pytorch, which from version 1.3 allows execution of the models on mobile devices. While this is promising development direction, it is still lacking the GPU and Andorid Neural Network API (NNAPI) support,  which is currently the major drawback. The variety of Android devices results in another problem. There are multiple device manufactures and chip manufactures, which causes slow adoption of the NNAPI. 

% Nie rozumiem. Trzeba to przepisac, bo jest zbyt zamotane. Lrotkie zdania, malo lania wody.
What is more the usage of NNAPI depends on the drivers provided by the device manufacturer. With no drivers provided it defaults to the use of CPU while on the other hand, there might be major differences between smartphones models in terms of supporting the NNAPI as some of them might support only quantized (INT8) or float (FP32) only models. All of that make it hard to predict how deployed neural network model will behave on the devices. As mentioned in Ignatov et. al\cite{aiMobileBenchmark} the newest generation of NPU seems to provide a wide, common set of supported operations while supporting both quantized and float models at the same time. All of that shows a great progress made in last years on the Android device which should made it easier to deploy deep learning model on both of the platforms as till now Android was inferior in comparison with iOS.


%While it was introduced to replace the previous Tensorflow Mobile there are still some limitations. 
%It allows to write delegates to custom hardware that is supporting
%\item CoreML support with the iPhone dedicated NPU - ease of use 
%Discuss the problems on Android platform which stopped us from deploying the application (yet) implementation in Pytorch instead of tensorflow
%problems with model conversion between formats (with regard to the performance side - added transpositions in the process)
%Pytorch lack of GPU/NNAPI support (for now)
%\item Tensorflow Lite limitations with GPU (limited set of operations)
%\item Tensorflow Lite policy about switching to CPU in case of unsupported  operation with GPU/NNAPI
%\item Lack of drivers and documentation of NNAPI as it heavily depended on the hardware (not all operations are supported, some chips needs DNN to be quantized and so on, reference the AI benchmark with 1-4th  NPU generati

\subsection{Investigation on size of network}


%\item Examples (without and with stabilisation) - videos(links) and photos



\begin{table*}[]
\centering
\scriptsize
\caption{Configurations tested}
\label{tab:my-table}
\begin{tabular}{rrrrrrrrr}
\hline
Id & $\alpha$ & $\beta$ & parameters & \% of ReCoNet & Size (MB) & \% of ReCoNet & FPS ($480\times320$) & FPS ($320\times240$)\\ \hline
1  & 1.000 & 1    & 469731     & 15.16         & 1.79     & 15.14         & 12.26       & 19.57       \\
2  & 0.750 & 1    & 267627     & 8.64          & 1.02     & 8.63          & 14.91       & 21.41       \\
3  & 0.500 & 1    & 121971     & 3.94          & 0.47     & 3.98          & 21.91       & 25.17       \\
4  & 0.250 & 1    & 32763      & 1.06          & 0.12     & 1.02          & 27.72       & 40.91       \\
5  & 0.125 & 1    & 9327       & 0.30          & 0.04     & 0.34          & 34.08       & 46.41       \\ \hline
6  & 1.000 & 0.75 & 307683     & 9.93          & 1.17      & 9.90          & 15.10        & 18.53       \\
7  & 0.750 & 0.75 & 173739     & 5.61          & 0.66      & 5.58          & 16.66       & 21.52       \\
8  & 0.500 & 0.75 & 77811      & 2.51          & 0.30       & 2.54          & 21.28       & 34.77       \\
9  & 0.250 & 0.75 & 19899      & 0.64          & 0.08      & 0.68          & 38.63       & 51.37       \\
10 & 0.125 & 0.75 & 5199       & 0.17          & 0.02      & 0.17          & 49.92       & 60.41       \\ \hline
11 & 1.000 & 0.5  & 233571     & 7.54          & 0.89      & 7.53          & 15.66       & 18.93       \\
12 & 0.750 & 0.5  & 131979     & 4.26          & 0.50       & 4.23          & 18.21       & 23.43       \\
13 & 0.500 & 0.5  & 59187      & 1.91          & 0.23      & 1.95          & 22.64       & 37.68       \\
14 & 0.250 & 0.5  & 15195      & 0.49          & 0.06      & 0.51          & 40.01       & 53.90        \\
15 & 0.125 & 0.5  & 3999       & 0.13          & 0.02      & 0.17          & 51.92       & 62.43     \\ \hline

\end{tabular}
\end{table*}




\begin{figure}[ht!]
\caption{A graph showing FPS versus number of parameters in DNN model tested with iPhone 11 Pro. The x-axis is presented in logarithmic scale with scale factor of $1000$}
\label{graphFPS}
\centering
\includegraphics[width=0.49\textwidth]{chart.png}
\end{figure}



In Tab. \ref{tableWithArchitecture} we introduced $\alpha$ and $\ResidualLayerNumber$  parameters for the number of filters and number of residual layers. We ran training sessions with these parameters, to check how they impact achieved style transfer. In Tab. \ref{tab:my-table} we present the results of our experiments. The qualitative (visual) comparison for the network trained with "Scream" by Munch style is presented in the Fig. \ref{fig:comparison} while the content and style image for these models are in Fig. \ref{fig:content_and_style}. All of the networks were trained with exactly same settings for hyperparameters.

As we can see in Fig. \ref{fig:comparison} the smallest networks (ids 5, 10 and 15) provided very poor results, with multiple deformations whihc made it hard to recognize the original content image. At the same time networks denoted with ids 4,9 and 14 provides much better results and were able to capture both: style and content. In our opinion these results indicate, there is a tradeoff between keeping good looking stylization and remaining proper content. We can speculate that model 14 is able to provide the most fine details (look at the clock tower of Big Ben) from those three while being the smallest, however this type of evaluation is purely subjective. W can also hypothesize that the model capacity of all the  networks from 4, 9 and 14 should be sufficient for learning the style, however achieving subjectively good results may depend on the initialization of network parameters. In some  cases, it may be difficult to recognize differences between the models. As there are some clear difference visible between them in our opinion the style and content is represented with the same level of detail and choosing one over another would be a matter of individual preferences about the certain style. 


No surprise, the reduction of the models size on disk is proportional to the reduction of parameters number. It ios well known, that it is parameters of networks to take the most space in saved model (see Tab. \ref{tableWithArchitecture}). 

We are also showing the reduction in both of those terms comparing it to ReCoNet \cite{Reconet}. The size of the network is a important matter when deploying it to mobile devices as the size of application may heavily depend on that. When each network provides a single artist style having multiple styles in one application can quickly grows its size. The original ReCoNet network size is $11.82$ MB while our experiments ranges form $1.79$ to $0.02$ MB. Thus we are able to provide multiple models while keeping smaller memory footprint. The performance of our model in terms of frames per second (FPS) were measured on the iphone 11 Pro with 2 different resolutions of the input for model. The measurement was done by applying the model to the video with resolution $1920\times1080$, length of $21$ seconds coded with H.264 and 25 FPS. The results present in Tab. \ref{tableWithArchitecture} are averaged speed of each model after processing this video (resizing image to network input size is included into this measurement). As we can see decreasing the number of parameters for network gives non linear growth in speed of processing. This trend is presented in the Fig. \ref{graphFPS}. We can see the logarithmic relation between the number of FPS and number of parameters. The difference between the models  1 and  2 is around $200 000$ parameters while this shrinking provides only 2 more FPS on averaged in favor of the smaller model. The further shrinking of models (ids 3, 8, 13) provides near real-time performance with 22 FPS on the $480\times320$ resolution and real-time performance (over 25 FPS) with $320\times240$ resolution. While we are not aware of the hardware design of the Apple chips what can be seen is that removing residual layers have rather small impact on the overall performance of the model while $\alpha$ is much more important. This can be especially seen when comparing models with $\alpha=1.0$ and different $\ResidualLayerNumber$. Despite great overall difference in the number of parameters between them the final performance is similar for $320\times240$ resolution. In this case we can see the fluctuation of measurement where smaller models show decresed performance as we were simulating real use case with some applications running in background. To conclude, we have proven that the size of the network, in terms of parameter numbers and the model size on the disk, can be greatly reduced, we are able to provide real-time performance on mobile devices without loosing expected result quality.  For the development of Heritage Portal application we choose the model with  $\alpha=0.5$ and $\ResidualLayerNumber=1$ which in our opinion provided nice looking results and sufficient performance while also significantly reduced size.




\begin{figure}[ht!]
%\begin{tabular}{p{.25\textwidth}p{.2\textwidth}}
\begin{tabular}{cc}

\IncG[width=.25\textwidth,height=.2\textwidth]{figures/content/ben.jpg} &
\IncG[width=.2\textwidth,height=.2\textwidth]{figures/style/scream.jpg} 
\\
a) Content image  & b) Style image 
\end{tabular}
\caption{ Content and style images for experiments in Fig. \ref{fig:comparison}}
\label{fig:content_and_style}
\end{figure}




%\begin{figure*}[ht]
%\begin{tabular}{p{.02\textwidth}p{.19\textwidth}p{.19\textwidth}p{.19\textwidth}p{.19\textwidth}p{.19\textwidth}}
%
%\hline
%$\beta$ &   \multicolumn{5}{c}{$\alpha$} 
%\\
%\hline
%& \multicolumn{1}{c}{1} & \multicolumn{1}{c}{0.75}   & \multicolumn{1}{c}{0.5} & \multicolumn{1}{c}{0.25}  & \multicolumn{1}{c}{0.125} 
%\\
%\rotatebox[origin=c]{90}{1} &
%\IncG[width=.19\textwidth,height=.19\textwidth]{figures/211_ben_800_600.jpg} &
%\IncG[width=.19\textwidth,height=.19\textwidth]{figures/212_ben_800_600.jpg} &
%\IncG[width=.19\textwidth,height=.19\textwidth]{figures/213_ben_800_600.jpg} &
%\IncG[width=.19\textwidth,height=.19\textwidth]{figures/214_ben_800_600.jpg} &
%\IncG[width=.19\textwidth,height=.19\textwidth]{figures/215_ben_800_600.jpg} 
%\\
%
%\rotatebox[origin=c]{90}{0.75} &
%\IncG[width=.19\textwidth,height=.19\textwidth]{figures/251_ben_800_600.jpg} &
%\IncG[width=.19\textwidth,height=.19\textwidth]{figures/252_ben_800_600.jpg} &
%\IncG[width=.19\textwidth,height=.19\textwidth]{figures/253_ben_800_600.jpg} &
%\IncG[width=.19\textwidth,height=.19\textwidth]{figures/254_ben_800_600.jpg} &
%\IncG[width=.19\textwidth,height=.19\textwidth]{figures/255_ben_800_600.jpg} 
%\\
%
%\rotatebox[origin=c]{90}{0.5} &
%\IncG[width=.19\textwidth,height=.19\textwidth]{figures/222_ben_800_600.jpg} &
%\IncG[width=.19\textwidth,height=.19\textwidth]{figures/223_ben_800_600.jpg} &
%\IncG[width=.19\textwidth,height=.19\textwidth]{figures/224_ben_800_600.jpg} &
%\IncG[width=.19\textwidth,height=.19\textwidth]{figures/225_ben_800_600.jpg} &
%\IncG[width=.19\textwidth,height=.19\textwidth]{figures/226_ben_800_600.jpg} 
%\\
%\hline
%\end{tabular}
%\caption{ \textcolor{red}{Poprawić tą tabelę, może pokazać 4 obrazki w rzędzie, na pewno przedstawić różne style}}
%\label{fig:comparison}
%\end{figure*}









\begin{figure*}[ht!]
%\begin{tabular}{p{.008\textwidth}p{.29\textwidth}p{.29\textwidth}p{.29\textwidth}}
\begin{tabular}{cccc}

\hline
$\alpha$ &   \multicolumn{3}{c}{$\beta$} 
\\
\hline
& \multicolumn{1}{c}{1} & \multicolumn{1}{c}{0.75}   & \multicolumn{1}{c}{0.5}  \\

\rotatebox[origin=c]{90}{1} &
\IncG[width=.3\textwidth,height=.215\textwidth]{figures/211_ben_800_600.jpg} &
\IncG[width=.3\textwidth,height=.215\textwidth]{figures/251_ben_800_600.jpg} &
\IncG[width=.3\textwidth,height=.215\textwidth]{figures/222_ben_800_600.jpg} 
\\

\rotatebox[origin=c]{90}{0.75} &
\IncG[width=.3\textwidth,height=.215\textwidth]{figures/212_ben_800_600.jpg} &
\IncG[width=.3\textwidth,height=.215\textwidth]{figures/252_ben_800_600.jpg} &
\IncG[width=.3\textwidth,height=.215\textwidth]{figures/223_ben_800_600.jpg}
\\

\rotatebox[origin=c]{90}{0.5} &
\IncG[width=.3\textwidth,height=.215\textwidth]{figures/213_ben_800_600.jpg} &
\IncG[width=.3\textwidth,height=.215\textwidth]{figures/253_ben_800_600.jpg} &
\IncG[width=.3\textwidth,height=.215\textwidth]{figures/224_ben_800_600.jpg} 
\\

\rotatebox[origin=c]{90}{0.25} &
\IncG[width=.3\textwidth,height=.215\textwidth]{figures/214_ben_800_600.jpg} &
\IncG[width=.3\textwidth,height=.215\textwidth]{figures/254_ben_800_600.jpg} &
\IncG[width=.3\textwidth,height=.215\textwidth]{figures/224_ben_800_600.jpg} 
\\

\rotatebox[origin=c]{90}{0.125} &
\IncG[width=.3\textwidth,height=.215\textwidth]{figures/215_ben_800_600.jpg} &
\IncG[width=.3\textwidth,height=.215\textwidth]{figures/255_ben_800_600.jpg} &
\IncG[width=.3\textwidth,height=.215\textwidth]{figures/226_ben_800_600.jpg} 
\\
\hline
\end{tabular}
\caption{ Qualitative comparison of stylization performed by networks with different size}
\label{fig:comparison}
\end{figure*}



\section{Conclusions}

% Ujednolicic czas w calej publikacji. Najlepiej past simple
In this paper, we present a feed-forward neural network based on ReCoNet \cite{Reconet} for real-time video style transfer on mobile devices. We showed technical challenges in deploying CNN to mobile devices and the differences between Android and iPhone ecosystem with recommendation for any future work. This includes that we need to be very careful about using certain layers and even kernel sizes in order to achieve expected performance. We also proposed novel way of achieving temporal consistency by re-training already available models. By this we made it easy to use models that are trained with other methods. At last we also investigated the network size regarding the number of filters and number of residual layers and showed that shrinking them to $3.94\%$ number of parameters present in \cite{Reconet} we are still able to achieve good looking results showed by experiments. The results of that work can be checked with Heritage Portal mobile application on iOS devices.


%\section*{Acknowledgments}
%
%W. Dudzik was co-financed by the European Union through the European Social Fund (grant POWR.03.02.00-00-I029)


\bibliographystyle{IEEEtran}
\bibliography{mybibliography}


\end{document}
